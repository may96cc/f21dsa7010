{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Carpentry with `Python`, part 2\n",
    "\n",
    "Recall our messy data from part one and all the manual steps we needed to clean things up.\n",
    "\n",
    "![title](../images/messy2.png)\n",
    "\n",
    "### The Data\n",
    "\n",
    "This is only a partial dataset on several [species of desert rodents](http://esapubs.org/archive/ecol/E090/118/Portal_rodent_metadata.htm). \n",
    "The files that we have used before have all been `csv`s, but `Python` can handle a lot more types of file formats. \n",
    "The file we will be working with today is in `.xls` format, otherwise known as an Excel file. \n",
    "\n",
    "To clean up this data, we are going to use Pandas and the `xlrd` (Excel Reader) library.\n",
    "  * External API Link: https://pypi.python.org/pypi/xlrd\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "\n",
    "# xlrd is a package for developers to extract data from\n",
    "# Excel spreadsheets. https://pypi.python.org/pypi/xlrd\n",
    "\n",
    "import pandas as pd \n",
    "import xlrd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the File with Pandas\n",
    "\n",
    "One thing to note here, we have yet to see the `header` parameter used. \n",
    "Normally, when a data frame object is created, the first row in the file is interpreted to be the header row, \n",
    "and often this is okay. \n",
    "But in its current state, our data is in no place to have a header row yet. \n",
    "Plus, the first row in the dataset for this Excel file is an empty line.\n",
    "When there aren't any data to be the header row, \n",
    "`pandas` defaults the names to a series of **`Unnamed:`**s or numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_excel('/dsa/data/all_datasets/messy_survey.xls', header = None)\n",
    "\n",
    "# now it is saved to the object `file`\n",
    "\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remember, we are essentially dealing with three different tables in the exact same file. \n",
    "\n",
    "You will notice all of the `NaN` values. \n",
    "This stands for \"Not a Number\" and is the default value for those cells that don't contain data.\n",
    "\n",
    "Recall our steps for file clean up:\n",
    "  1. Remove _no data_ columns\n",
    "  1. Add species column, and fill down\n",
    "  1. Remove non-header rows\n",
    "  1. Remove rows between sub-tables to create the unified table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1:  Remove 'no data' columns\n",
    "\n",
    "These are the first two columns of our data.\n",
    "We can do this by selecting out only the columns with data.\n",
    "Column and row indexes start at 0; and with a little `pandas` knowledge we can select out a range of columns.\n",
    "\n",
    "This is done with the `.iloc[ rows , columns ]` syntax.\n",
    "\n",
    "**Example**\n",
    "```\n",
    "file.iloc[ 0: , 2: ]\n",
    "```\n",
    "In this case, we are asking for all rows because it is \"`0:`\", and columns numbered 2 to the end with \"`2:`\"\n",
    "\n",
    "\n",
    "**Note:** the `.head()` function below just shows the top few rows in the Pandas data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.iloc[ 0:, 2: ].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "```\n",
    "file.iloc[ 5: , 0: ]\n",
    "```\n",
    "In this case, we are asking for all columns because it is \"`0:`\", and rows numbered 5 to the end with \"`5:`\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.iloc[ 5: , 0: ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting Indexes after selection\n",
    "The `.reset_index(drop = True)` function on the Pandas data frame will reset the indexes on the rows.\n",
    "Notice the output above, the first row listed is indexed as **5**.\n",
    "\n",
    "And below, we can _fix_ it to be back to 0-based indexing for the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.iloc[ 5: , 0: ].reset_index(drop = True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait weren't we trying to complete step 1?\n",
    "**Notice that most of Step 3 was just accomplished!**\n",
    "\n",
    "## Combining step 1 & partial-step 3 of our process!\n",
    "\n",
    "To keep our source `file` data as-is, we will cut the columns and create a new variable with the result.\n",
    "We are going to call this modified data frame **`messy`**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new variable\n",
    "messy = file.iloc[5:,2:].reset_index(drop = True)\n",
    "messy.head(20)  # option number of rows to show in head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Add species column, and fill down\n",
    "\n",
    "So this next section may be a little complicated, particularly if you are new to programming, \n",
    "but we will take this step-by-step.\n",
    "\n",
    "First we will define the species cell values, which we saw are:\n",
    "  * `Species: DM`\n",
    "  * `Species: DO`\n",
    "  * `Species: DS`\n",
    "\n",
    "We will put these into a list as the names of the sub tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = [\"Species: DM\", \"Species: DO\", \"Species: DS\"]\n",
    "table_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you learn programming, you learn tricks and patterns to solve certain types of problems.\n",
    "The need to fill a value down to a point then reset to a new value is a common pattern of tasks.\n",
    "\n",
    "Notice above that our column index 2 (heading does not reset like rows) holds the sub-table names we put into our list. Lets investigate that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `.isin()` function of the Pandas data frame to test if each value is in a list of values.\n",
    "Recall our list of values:\n",
    "```\n",
    "['Species: DM', 'Species: DO', 'Species: DS']\n",
    "```\n",
    "\n",
    "This gives us a list of True or False values going down the column based on the existence of the value in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy[2].isin(table_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So... \n",
    "\n",
    "What can we do with this list of values?\n",
    "Well, as it happens, there are a couple of rules of computers that work to our advantage.\n",
    "  * `True` = 1\n",
    "  * `False` = 0\n",
    "  \n",
    "This allows us to process this `True/False` column with another function, _cumulative summation_, `.cumsum()`.\n",
    "\n",
    "So, adding True and False is `1 + 0` and if we keep accumulating down the column, every time we hit `True`, the value increases by 1.\n",
    "\n",
    "A couple of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([True, False, True]).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([True, False, True, False, True, True, True, False, False]).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Therefore... \n",
    "\n",
    "We can use this technique to apply labels, where group 1 is all the `Species: DM`, 2 is `Species: DO`, and 3 is `Species: DS`. This can be done because of the shape of our data. If you recall `Species: DM` is a \"table\" with the excel file then `Species: DO` comes next and finally `Species: DS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture this processing (label creation) as a new variable, groups\n",
    "groups = messy[2].isin(table_names).cumsum()\n",
    "print(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have list of values we can use to label the rows.\n",
    "We can use the Pandas data frame `.groupby()` function to partition a data frame into sets of rows associated with a column value.\n",
    "\n",
    "In this case, the rows that allign to the number 1 will be one group, 2 another, and 3 a third group of rows.\n",
    "\n",
    "The result is going to be a new type of GroupBy data frame:\n",
    "```\n",
    "pandas.core.groupby.DataFrameGroupBy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messy.groupby(groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will expand back out the data frame Group By object to show you the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each element in set\n",
    "# k     g\n",
    "# 1 -> { row 0 , row 12}\n",
    "# 2 -> { row 13 , row 21}\n",
    "# 3 -> {}\n",
    "for k,g in messy.groupby(groups):\n",
    "    print(\"k = {}, g = {}\".format(k,g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice that ... \n",
    "We have now set up the data into 3 sub-tables, K = 1, 2, and 3.\n",
    "\n",
    "So, let's use some more appropriate variable names:\n",
    "  * `species`  as the index from the cumsum() function.\n",
    "  * `rows` as the collection of rows that have same label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species,rows in messy.groupby(groups):\n",
    "    print(\"species = {}, rows = {}\".format(species,rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at the rows... \n",
    "The first row and column of each sub-table has the Species name.\n",
    "We can access the cell with the Pandas data frame `.iloc[row,col]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species,rows in messy.groupby(groups):\n",
    "    print(rows.iloc[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can look at all the rows from 1 to the end with the `.iloc[rows]`, single argument function.\n",
    "\n",
    "#### NOTE: This completes step 3 by removing the Species name row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species,rows in messy.groupby(groups):\n",
    "    print(rows.iloc[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these two manipulations, we can create a `Key->Value` data structure, called a **dictionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {} # an empty dictionary variable named tables\n",
    "\n",
    "for species,rows in messy.groupby(groups):  # For each species and its rows:\n",
    "    tables[rows.iloc[0,0]] = rows.iloc[1:]  # store into the tables variable\n",
    "   \n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In Python, there is always an alternative way to accomplish a task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This creates the same dictionary as above, using a \"dictionary comprehension\" (aka fancy Python)\n",
    "tables = {rows.iloc[0,0]: rows.iloc[1:] for species,rows in messy.groupby(groups)}\n",
    "\n",
    "############\n",
    "# t={v.iloc[0,0]: v.iloc[1:] for k,v in m.groupby(g)}\n",
    "############\n",
    "\n",
    "\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do these tables look like now?\n",
    "\n",
    "Let's take a pause in our cleaning process, and print out what these separated tables look like now. We will create a for loop that iterates over each table, prints out its name and then the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for species, tab in tables.items():  # pull out each Key->Value pair in the tables variable\n",
    "    print(\"table:\", species)\n",
    "    print(tab)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that each table is now a dictionary entry, so in the `for` loop we specify **`for species, tab in tables.items():`**. \n",
    "\n",
    "`species` stands for key, \n",
    "`tab` for value. \n",
    "\n",
    "This is how you iterate over dictionaries. \n",
    "\n",
    "We then print the `species` value first, then the `tab` variable (the data within the table). \n",
    "The last `print` statement just creates a space between one table and the next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataframe\n",
    "\n",
    "Now is the time to put the pieces back together. We really want to create a single data frame and create a new variable specifying the species. Take a look at how we use a for loop below to iterate over `table.items()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [] # an empty list where we will store our separate dataframes\n",
    "\n",
    "for species, tab in tables.items(): # iterate over the table dictionaries\n",
    "    \n",
    "#    single_frame = pd.DataFrame(tab) # create a dataframe from each table \n",
    "    \n",
    "    single_frame = pd.DataFrame(tables[species]) # create a dataframe from each table \n",
    "    \n",
    "    single_frame['species'] = species # create a new column called species and fill it with the name of the data frame\n",
    "    \n",
    "    single_frame = single_frame.reset_index(drop=True).iloc[1:] # reset index and remove first row (header row)\n",
    "    \n",
    "    single_frame.columns = ['date','plot','sex','weight','species'] # rename the columns\n",
    "    \n",
    "    dfs.append(single_frame) # add to the list of separate data frames\n",
    "\n",
    "df = pd.concat(dfs).reset_index(drop = True) # join the dataframes together into one data frame\n",
    "df # return the complete data frame    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of steps in this for loop. For that reason, each line of code is commented to describe what each piece is doing. \n",
    "\n",
    "We are now in pretty good shape, but there are still some things we need to do before we can consider this tidy. One thing that should be noticeable right of the bat is there are rows of data that are almost completely `NaN` values, save for the `Species` column. Why is that? \n",
    "\n",
    "Let's take a look at the picture again...\n",
    "\n",
    "![title](../images/messy2.png)\n",
    "\n",
    "Notice that between each table there is a blank row. That is the very reason for these lines. Essentially, these are residual from our initial reading in of the file and they have remained throughout the process. Well, now it is time to get rid of them.\n",
    "\n",
    "For this, we will use the Pandas `.dropna()` function to remove rows that have NaN values, then reset the row indexing as we did previously.\n",
    "\n",
    "**Example** \n",
    "```\n",
    "df.dropna().reset_index(drop = True)\n",
    "```\n",
    "However, if we just drop the `NaN` rows, we will lose some of the Species DM rows that have weight blank.\n",
    "Instead, we need to limit the `NaN` check to the `date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-assign the result of removing NaN date rows and re-index the rows of the data frame\n",
    "df = df.dropna(subset = ['date']).reset_index(drop = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Clean Up\n",
    "\n",
    "We have achieved most of carpentry goal, now we just need to reform the species column.\n",
    "Having \"Species:\" repeated for every single value is unnecessary. \n",
    "We are going to go ahead and remove this segment, leaving only the species abbreviations.\n",
    "\n",
    "Again, there are several ways to do this, but the method below takes on a familiar format that we have seen before. \n",
    "\n",
    "Note: The code below relies on \n",
    "  1. String `.split()` to convert a string into a list of strings\n",
    "  1. List `.pop()` to remove and return the last item in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbrv = []  # Start an empty list\n",
    "\n",
    "# for every row in the species column\n",
    "for i in df.species:\n",
    "    # Append to the list\n",
    "    abbrv.append(\n",
    "        i.split(\" \").pop() # the result of splitting the Species: DX into two elements, taking the first element away.\n",
    "    )\n",
    "    \n",
    "# The above for loop produced a list of Species, make that list the new species column\n",
    "df['species'] = abbrv\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the `df['weight']` column. \n",
    "This is a variable that we would like to find some stats about, but in its current state we can't do that. \n",
    "\n",
    "Take a look at the column again. \n",
    "Some of the values have a \"g\" added to the number indicating grams. \n",
    "It is this \"g\" that we want to remove so that we can start running some stats on it. \n",
    "Below is one way to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for this we need to import the numpy package (numerical python)\n",
    "import numpy as np\n",
    "\n",
    "# Start an empty list\n",
    "nums = []\n",
    "# for each value in the column\n",
    "for i in df.weight:\n",
    "    # if the value i, pulled from the column is empty as a number\n",
    "    if pd.isnull(i):\n",
    "        val = np.nan # set the temporary variable to Not a Number\n",
    "    else:\n",
    "        # Other wise join all the digits into a single string in the temporary val variable\n",
    "        val = ''.join(c for c in str(i) if c.isdigit())\n",
    "    # Add the variable into the list, possible as NaN or a number.\n",
    "    nums.append(val)\n",
    "    \n",
    "# Assing this column of clean variables for weight to the weight column\n",
    "df['weight'] = nums\n",
    "\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the \"g\" characters are removed.\n",
    "\n",
    "The method is similar to how we modified the `species` column except we added a conditional. The reason we need the conditional is because `NaN` values will halt our for loop and we don't want to get rid of them, so we first need to check if the value is `NaN`. If the values are `NaN`, we will return them as `NaN`, otherwise, we use this line of code to take only those values that are numbers and return them.\n",
    "\n",
    "#### But there is one problem... \n",
    "Our column is still a `String` type, and we cannot compute a numerical mean on a string. \n",
    "\n",
    "Therefore, we will convert it to a float using the Numerical Python library (NumPy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'] = df['weight'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what to do about those pesky `NaN`s? Well, we could remove rows with those values, or we can fill them in. Below is how you fill them in using the mean weight, this way we are using the average of the dataset.\n",
    "\n",
    "But let's take a look at the column again. We will copy the data frame to a frame called `cleaned`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = df\n",
    "cleaned['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned['weight'] = cleaned['weight'].fillna(cleaned['weight'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how you would fill in the `NaN` values with the mean weight. \n",
    "This allows us to keep the information of the other columns, which may be useful, \n",
    "but not affect the overall mean of the weight column. \n",
    "Take a look at our final frame below.\n",
    "\n",
    "**To think about:** Is there a better way to fill in these `NaN`s?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Carpentry is sometimes tedious to construct as a process for data cleaning\n",
    "\n",
    "However, remember that the alternative was to clean 100's or 1000's of those files by hand (with Excel or text editor).\n",
    "\n",
    "By developing a data transformation and cleaning script, we can ingest any number of these files for further analytical inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
