{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Soft Introduction to Machine Learning with Naïve Bayes\n",
    "\n",
    "For these next two modules, we will be dipping our feet into the metaphorical ocean that is machine learning. This module is meant to familiarize you with the packages and tools to get you started making some prediction given a set of data. \n",
    "\n",
    "This week we will be working with two particular algorithms that will classify some target given a set of inputs: Naïve Bayes and Decision Trees. We are going to provide the methods to allow us to hit the ground running with these algorithms and the complimentary data preparation in order to make more accurate models. Keep in mind that there are some more abstract mathematical concepts under the hood of these functions, but for the purpose of these two modules, we are going to de-emphasize the math involved and focus on how to use the tools. This is not to say that the math is unimportant, quite the opposite in fact, and the concepts will be discussed in further detail in the Statistical and Mathematical Foundations course as well as throughout the curriculum. \n",
    "\n",
    "For this module and the next, we will only be discussing four such algorithms and when they are appropriate to use. Naïve Bayes and Decision Trees, the two we will be using this module, are not as complex as many of the others, but they are still widely used in the machine learning community. Why? Because they work surprisingly well when making decisions.\n",
    "\n",
    "Let's begin by reading in some of the dependencies that we will need for this week..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some carpentry...\n",
    "\n",
    "Remember the `iris` dataset that we used in the `ggplot2` lessons? Well, `sklearn` provides a copy of the dataset too, which is why we call that third line:\n",
    "```python\n",
    "from sklearn import datasets\n",
    "```\n",
    "This gives us access to some preloaded data that we can begin to play with. Today, we are going to be using the `iris` dataset again, because we are already familiar with it. Take a look at what it looks like..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! It looks strange! Well, that is because the `datasets` module stores its datasets as dictionary objects with different components of the dataset stored as key-value pairs. What we are interested in is the values of the `data` key as well as the `target` key. Take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() # load the iris dataset from sklearn\n",
    "data = pd.DataFrame(data=iris.data) # create frame of input data\n",
    "target = pd.DataFrame(data=iris.target) # create frame of target data\n",
    "\n",
    "df = pd.concat((data,target), axis=1) # combine input and target together\n",
    "col_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'] # column names\n",
    "df.columns = col_names # name data frame columns\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the `iris.data` references the values for the input data of the `iris` data frame, in other words the variables sepal length and width, and petal length and width. The `iris.target` stores the values of the target variable (the variable we will attempt to predict) or, in this case, the `species` variable. These values are stored as `numpy` arrays, so we transform them into `pandas` data frame objects. We then combine these objects together into one data frame, which we call `df`. \n",
    "\n",
    "Oh, but one more bit of carpentry. We are going to go ahead and transform the values in the `species` column to the species names.\n",
    "    \n",
    "\\* *This is purely for easier representation of the data. You can actually skip this step in the learning process, but data frames make it easier to visualize what we are working with.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_to_replace = {0:'setosa', 1:'versicolor', 2:'verginica'}\n",
    "df['species'] = df['species'].map(vals_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to change the values from integers to strings, we went ahead and created a dictionary where the original integer was the key and the species name was the value. This way, we could just map the dictionary to the column where it finds those values that match the key and replaces them with the values. Here is what we have below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes Classification\n",
    "\n",
    "Naïve Bayes classification is a popular machine learning algorithm for prediction and works particularly well on large datasets and, in many cases, will outperform some of the most complex algorithms. Let's see if we can use it to predict the species of iris given the measurements taken.\n",
    "\n",
    "First, we have to import the function that will perform the task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nbc = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it. We imported a Gaussian Naïve Bayes and we renamed the function nbc. Now we need to define our input variables, we will call these `X` and our target, `y`. Remember, our inputs are the measurements and our target is the species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = np.asarray(df.species)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go. From here, we want our model to predict our target variables given a set of inputs. To do this we must train our model by giving the data objects we just defined above. We do this by calling the method `fit()`...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it!...kind of. But aren't we missing a couple things? For example, what if we had some new points that we wanted to predict? How do we do that? \n",
    "\n",
    "Imagine that someone plopped an iris down at your desk and asked you what species it was. Well, they are asking the wrong person because you know nothing about irises...BUT you have a model that does. So you measure the plant dimensions and you come out with this list:\n",
    "\n",
    "* sepal lenght: 5\n",
    "* sepal width: 3\n",
    "* petal length: 2\n",
    "* petal width: .5\n",
    "\n",
    "You can now take this and plug these measurements into your model and predict the species that is in front you. It's as simple as calling `nbc.predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nbc.predict([[5, 3,2,.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"SETOSA!\" we tell our curious colleague. But wait...how do we know that our model was good? Well, it would be nice to assess our model. How could we do this though? Well we can find out how many targets it misclassified.\n",
    "\n",
    "We can create another array of data of the predictions of the inputs we fed our model in the first place. We can call this array `y_pred`. We can then find how many points from our target variable `y` do NOT match our `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nbc.fit(X, y).predict(X)\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\"\n",
    "      .format(iris.data.shape[0],(y != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby! Only 6 misclassified points! Or in other words..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - ((y != y_pred).sum()/iris.data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "96% of the points were classified correctly!\n",
    "\n",
    "We can now see what points were incorrectly classified. There are many ways to do this, but let's see it in data frame format, since it is organized and easy to visualize all the variables at once. To do that, we have to create some new columns. One column that returns the predicted species (`y_pred`) and the other that displays whether or not the prediction was correct (either `True` or `False`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['evaluation'] = y_pred == y\n",
    "df['pred'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was simple enough. Now we can filter the data frame where the `evaluation` column values equal `False` to see what it misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['evaluation']==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that some of the variability of versicolor and verginica cross a bit and therefore we have a model misclassifying some points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To come...\n",
    "\n",
    "So, we learned a little bit about machine learning in this notebook using a Naïve Bayes Classifier. However, there are several things we should take into consideration when constructing a model. What are the concepts of training and testing data, and what is the difference between the two? How do we know that all of the features are contributing to the model? Can we refine our model by selecting only certain variables? During the practices, we will tackle these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your noteboot, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
