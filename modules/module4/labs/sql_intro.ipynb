{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Acquainted with SQL\n",
    "\n",
    "Now that we are familiar with data carpentry and data types, we should familiarize ourselves with databases and how to interact with them. We will explore the basic concepts of SQL (structured query language) and database configuration and by the end of the module we will understand the power and portability of databases.\n",
    "\n",
    "The purpose of this notebook is to get our feet wet with SQL. We are particularly interested in the components that make up a query. Below, you will be exposed to data loading, but this portion will be deemphasized in order for us to get into the basics of constructing an SQL query. Don't worry, however, the next notebook will get into the some weeds of loading data into a database. With all of this being just a sampling of what is to come in the Database course. \n",
    "\n",
    "But why? Why use a database? Some technical reasons include:\n",
    "1. You can pull data out of them fairly simply\n",
    "2. They have structure and can store multiple tables \n",
    "3. Tables can easily be related to one another \n",
    "4. They can store large amounts of data\n",
    "5. They are concurrent; they allow for multiple users to access them at the same time\n",
    "6. They are portable and scalable\n",
    "\n",
    "So far in this course we have only be exposed to `Python` and `R` so is it really necessary to learn `SQL`?\n",
    "The simple answer to this is yes. `SQL` is the language of databases and a lot (most) of companies store their data in databases. \n",
    "\n",
    "There are several different flavors of databases. \n",
    "You may have heard of some: mysql, postgresql, Oracle, Microsoft SQL Server. \n",
    "In this notebook we will be working with SQLite. \n",
    "As its name suggest, SQLite doesn't have all of the features as the other systems, but it does adhere to the SQL language and is easy to get up and running. \n",
    "For that reason, we are starting here.\n",
    "\n",
    "Let's dive in..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = pd.DataFrame({\n",
    "    'name': ['Natalie Portman','Bill Murray', 'Will Smith', 'Kathy Bates'],\n",
    "    'age' : [35,66, 48, 69],\n",
    "    'birthplace' : ['Jerusalem, Israel', 'Evanston, Illinois', 'Philadelphia, Pennsylvania', 'Memphis, Tennessee']\n",
    "             })\n",
    "\n",
    "salary = pd.DataFrame({\n",
    "    'name': ['Natalie Portman', 'Natalie Portman', 'Bill Murray', 'Bill Murray','Will Smith','Will Smith','Kathy Bates', 'Kathy Bates', 'Viola Davis'],\n",
    "    'year': [2000, 2017, 2000, 2017, 2000, 2017, 2000, 2017, 2017 ],\n",
    "    'salary': [500000, 10000000, 1000000, 5000000, 750000, 10000000, 1200000,9000000, 8000000]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing different or new here. The one thing to keep in mind is that we are working with dummy data. The validity of these data points cannot be verified; the salary column is a straight up fabrication. You may realize either from context clues or knowledge of pop culture that we are dealing with actors and actresses. Whether or not you are familiar with these four is irrelevant. \n",
    "\n",
    "We just created two data frames. Why two? Well, we can reduce data redundancies when we split these up into two different tables. In other words, the attributes of the `actor` data frame would have to be repeated twice if joined with the `salary` table since it keeps record of an actor's salary for both the year 2000 and 2017. This concept is called `normalization` and will be covered more in the database course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(actor)\n",
    "print(\"---------\")\n",
    "print(salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below is where we create a database called `sample.db`. It exists in the directory above the directory we are currently in, meaning in the module sub directory and not the labs sub directory. \n",
    "\n",
    "Now, I don't want to get into the thick of what is going on here, primarily because we will cover this in the next lab. Just know that we are using `pandas` to create tables and then populate those tables with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_file_name = '../sample.db'\n",
    "\n",
    "if os.path.exists(db_file_name):\n",
    "    os.remove(db_file_name)\n",
    "    \n",
    "# Open / Create the sample.db database file.\n",
    "connection = sqlite3.connect(db_file_name)\n",
    "\n",
    "# Note: Each command below inspects the data and creates a table, \n",
    "# then populates the data into the table\n",
    "salary.to_sql('salary', connection, if_exists='replace')\n",
    "actor.to_sql('actor', connection, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Database\n",
    "\n",
    "Our data is now in a SQLite database...now what? Access it! \n",
    "\n",
    "Now keep in mind that this is just one way to access data from a database but you can connect to a database through many languages including `R`. Below is just a simple way to open a database connection and run a query and return the results to a `pandas` data frame. The important part below is the query (the stuff in quotes). \n",
    "\n",
    "#### `SELECT`\n",
    "\n",
    "The most basic component of querying in SQL is the `select` statement. You use this statement to tell the database which attributes (columns) you are interested in. After you specify the attributes, you tell SQL which table these attributes are from. Imagine that I wanted to see all of the values from the all of the attributes in the salary table. You would write\n",
    "\n",
    "```SQL\n",
    "SELECT *\n",
    "FROM salary;\n",
    "```\n",
    "\n",
    "The `*` tells SQL that you want all the attributes. Take a look for yourself by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query(\"SELECT * FROM salary;\", connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we understand what is going on, `pandas` has a function called `read_sql_query` that takes in a string containing a SQL query as the first argument and the database connection as the second. We open this connection two code cells above with this line:\n",
    "\n",
    "```python\n",
    "connection = sqlite3.connect(db_file_name)\n",
    "```\n",
    "\n",
    "Where `db_file_name` is the database file. This function then returns the results to a `pandas` dataframe.\n",
    "\n",
    "We can also specify a single column to pull like so\n",
    "\n",
    "```SQL\n",
    "SELECT <column_name>\n",
    "FROM <table_name>;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT name FROM salary;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or multiple columns\n",
    "\n",
    "```SQL\n",
    "SELECT <column_name> , <column_name>\n",
    "FROM <table_name>;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT name, year FROM salary;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `WHERE`\n",
    "\n",
    "The `select` statement allows us to subset a table columnwise, but what if we want a rowwise subset? The `where` clause achieves that. The `where` statement allows us to put a condition on the attributes so that we only return rows that meet that condition.\n",
    "\n",
    "```SQL\n",
    "SELECT <column_names>\n",
    "FROM <table_name>\n",
    "WHERE <column_name> [=<>!] <condition>;\n",
    "```\n",
    "\n",
    "For instances, if we only wanted rows from the salary table where the `year` is 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM salary WHERE year = 2017;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `ORDER BY`\n",
    "\n",
    "Sometimes you may just want to see what the data looks like from an ordered perspective. For example, a very reasonable thing to want to know is who had what salary from least to greatest during the year of 2017. \n",
    "\n",
    "```SQL\n",
    "SELECT <column_name>\n",
    "FROM <table_name>\n",
    "ORDER BY <column_name>;\n",
    "```\n",
    "\n",
    "*Again, remember these data are made up for the purpose of demonstration. I have no idea what these actors make although they definitely made more than yours truly did in my acting career *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM salary WHERE year = 2017 ORDER BY salary;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and perhaps you wanted to know the order from greatest to least. We didn't need to specify an order for the least to greatest as the default order is ascending. However, we could have by placing `ASC` after the column. As you can see below for descending order (greatest to least) we use `DESC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM salary WHERE year = 2017 ORDER BY salary DESC;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you think will happen if you order a query by a non-numeric attribute?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with non-numericattribute ordering here\n",
    "# ------------------\n",
    "\n",
    "query = \"SELECT name FROM salary WHERE year = 2017 ORDER BY name;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `JOIN`s\n",
    "\n",
    "Joins are among SQL's most powerful features. Joins allow us to relate one table to another, returning the results to us as a single table. Now, `SQLite` doesn't include all the different types of `joins` but it certainly has enough to display the concept's benefits. \n",
    "\n",
    "#### `CROSS JOIN`\n",
    "\n",
    "`CROSS JOIN`ing one table to another joins every row in table A to every row in table B. **A WARNING ABOUT `CROSS JOIN`S**: they have the tendency to get very large so use with caution. For our dummy case today we should only expect a table of 36 rows X 8 columns ( (num rows in A X num rows in B) **BY** (num cols in A + num cols in B) ).\n",
    "\n",
    "![CrossJoin](../images/crossJoin.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM actor CROSS JOIN salary\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `INNER JOIN`\n",
    "\n",
    "`INNER JOIN`s are going to return only those rows that have a match in both tables. We can specify which column we want to join by in `SQLite` by using the `USING` statement. In the case below we are going to join by `name` as this is the only feature that exists in both tables. We can expect that all rows that have a matching `name` in both tables will be returned, therefore, all actors save for Viola Davis will be returned to us. Viola Davis doesn't have a row in the `actor` table.\n",
    "\n",
    "\n",
    "![ven-inner-join](../images/sql-inner-join.png)\n",
    "\n",
    "![inner join](../images/innerJoin.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM actor INNER JOIN salary USING (name);\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can reverse the table order for this query to see what behavior occurs. You will notice that the table mentioned first will have its attributes displayed on the left..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM salary INNER JOIN actor USING (name);\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `OUTER JOINS`\n",
    "\n",
    "There are several different types of `OUTER JOIN`s but `SQLite` only supports one -  `LEFT OUTER JOIN`. `OUTER JOINS` are going to match on a specific column but will return Null values if there isn't a match. A `LEFT OUTER JOIN` will ensure that all of the columns on the left side table (the table listed first in the query) are present regardless of whether or not there is an analogous value in the matching attribute column. \n",
    "\n",
    "Again, let's specify `name` as our matching column. Instead of using `USING`, you can also use `ON` and specify the table and column name that match using the `tablename`.`attributename` syntax seen below.\n",
    "\n",
    "![VenouterJoin](../images/sql-left-join.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM actor LEFT OUTER JOIN salary ON actor.name = salary.name;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look familiar. In fact, it is the same result that we got for the first `INNER JOIN` that we committed above. That is because the `salary` table contains all of the names that are found in the `actor` table. However, what if the `salary` table is on the left?\n",
    "\n",
    "![Left Outer Join](../images/LeftOuterJoin.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"SELECT * FROM salary LEFT OUTER JOIN actor ON actor.name = salary.name;\"\n",
    "pd.read_sql_query(query, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go. You might have guessed it. Since Viola Davis doesn't have any rows in the `actor` table, the values under the attributes brought over from the `actor` table have `NaN` values assigned to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
