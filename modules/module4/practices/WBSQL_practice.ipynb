{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database (SQL) Mini-Analysis of World Bank Data - Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Much has been written about the potential impact the use of cell phones while driving have on traffic injuries and fatalities worldwide (see references below). This mini-analysis performs an initial exploratory investigation of the relationship between international cell phone subscriptions and mortality from traffic injuries for the years 2000 and 2019. The data were obtained from the World Bank free online dataset repository.\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "The learning goal of this practice is to become proficient with applying data carpentry and shaping and basic exploratory data analysis to the creation and manipulation of an SQL database.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "* Read .csv files into Pandas data frames\n",
    "* Construct and execute SQL \"CREAT TABLE\", \"Insert Into\", and \"SELECT\" statements consistent with specified data requirements\n",
    "* Apply data carpentry to (create/drop/revise) Pandas rows and columns\n",
    "* Reshape Pandas data frame structures from \"wide to long\" and \"long to wide\" using .melt() and .pivot() methods\n",
    "* Compute bivariate correlations and plot data values useful for exploratory data analysis\n",
    "\n",
    "## Resources\n",
    "\n",
    "To begin, read and become familiar with the details of the [World Bank Data Dictionary](../resources/WorldBankDataDictionary.pdf) describing the necessary requirements for constructing the database and necessary table schema and table relationships.\n",
    "Also refer to initial module technical readings as necessary.\n",
    "\n",
    "\n",
    "#### References\n",
    "- [World Bank Data Repository](https://data.worldbank.org/)\n",
    "\n",
    "- [Association between Cellular-Telephone Calls and Motor Vehicle Collisions](../resources/AssociationbetweenCellular-TelephoneCallsandMotorVehicleCollisions.pdf)\n",
    "\n",
    "- [Mobile Device Use While Driving â€” United States and Seven European Countries, 2011.pdf](../resources/MobileDeviceUseWhileDrivingUnitedStatesandSevenEuropeanCountries2011.pdf)\n",
    "\n",
    "- [WHO-RoadSafetyReport2018.pdf](../resources/WHO-RoadSafetyReport2018.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "\n",
    "After making any required code changes and completing the tasks and activities, run each cell individually in sequence to execute the code and obtain output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inport all the necessary libraries/packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "pd.set_option(\"max_colwidth\", 100) #<--make pandas columns wider\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is SQLite3?\n",
    "From the Documentation:\n",
    "\n",
    "\"SQLite is an in-process library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. The code for SQLite is in the public domain and is thus free for use for any purpose, commercial or private. SQLite is the most widely deployed database in the world with more applications than we can count, including several high-profile projects.\n",
    "\n",
    "\"SQLite is an embedded SQL database engine. Unlike most other SQL databases, SQLite does not have a separate server process. SQLite reads and writes directly to ordinary disk files. A complete SQL database with multiple tables, indices, triggers, and views, is contained in a single disk file. The database file format is cross-platform - you can freely copy a database between 32-bit and 64-bit systems or between big-endian and little-endian architectures. These features make SQLite a popular choice as an Application File Format. Think of SQLite not as a replacement for Oracle but as a replacement for fopen().\n",
    "\n",
    "### SQLite Interfaces:\n",
    "SQLite has numerous interfaces, including a command line interface (CLI). Additionally, API interfaces are provided in numerous environments / langauges such as C / C++ / Python/ R / PHP / Perl /......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to activate the function.  This utility function will be used in other cells to retrieve and display the constructed database structure and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_db_table_structure(conn):\n",
    "    '''\n",
    "    This function will output as text the names of each table in the active database connection\n",
    "    and each column deail in each table.\n",
    "        \n",
    "    Arguments:\n",
    "       conn: the active/open database connection.\n",
    "    \n",
    "    Returns : nothing\n",
    "    '''\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "        print(\"WBIndicatorDB - Tables\")\n",
    "        print(tables)\n",
    "        print(\"WBIndicatorDB - Table Schema\")\n",
    "        for t in tables:\n",
    "            print(t[0])\n",
    "            print(cursor.execute(\"PRAGMA table_info('\" + t[0] +\"')\").fetchall())\n",
    "            \n",
    "    except Error as e:\n",
    "        print(\"display_db_table_structure -- Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the Pandas \"read_csv\" method to read in each of the three World Bank comma separated value (csv) files and assign to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocodes_df = pd.read_csv('/dsa/data/all_datasets/world_bank_datasets/WBGeoCodes.csv') #path = /dsa/data/all_datasets/world_bank_datasets/...\n",
    "CellPhones_df = pd.read_csv('/dsa/data/all_datasets/world_bank_datasets/WBCellPhoneUse.csv')\n",
    "TrafficMort_df = pd.read_csv('/dsa/data/all_datasets/world_bank_datasets/WBTrafficFatalities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the head of each data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(geocodes_df.head())\n",
    "display(CellPhones_df.head())\n",
    "display(TrafficMort_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Activity 1:** Using the database dictionary, fill-in the necessary details for constructing an SQL **CREATE TABLE** statement string for creating the WBGeoCodes table. Be sure to specify each column name and data type and set the primary key.  Starter code is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Activity 1\n",
    "# --------------\n",
    "\n",
    "#geocodes_CreateTableStmt = \"CREATE TABLE ??? ( \"\n",
    "#geocodes_CreateTableStmt += \"???\"\n",
    "#geocodes_CreateTableStmt += \")\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After constructing the SQL CREATE TABLE statement for the geocode data, we can now create the table in which those data will be stored. All database operations will be encapsulated in a try/except block in order to manage and debug any potential exception conditions that might occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #Open and connection to the database -- if database is not present it will be created, otherwise it will be opened but not replaced\n",
    "    databaseFilename = 'WBIndicatorsDB01.db'\n",
    "    connection = sqlite3.connect(databaseFilename)\n",
    "    \n",
    "    #Instantiate a cursor object intended for database traversal\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    #Execute a DROP TABLE for the WBGeoCodes to allow new empty table to be created\n",
    "    cursor.execute(\"DROP TABLE if exists WBGeoCodes;\")\n",
    "    \n",
    "    #Exeute our CREAT TABLE statement\n",
    "    cursor.execute(geocodes_CreateTableStmt) #<-reference to the create table statement used to construct table.\n",
    "    \n",
    "    #Commit or save our change to the database\n",
    "    connection.commit()\n",
    "    \n",
    "    #Display table structure/schema\n",
    "    display_db_table_structure(connection)\n",
    "    \n",
    "    #Close the database connection\n",
    "    connection.close()\n",
    "    print(\"Database and WBGeoCode table created.\")\n",
    "\n",
    "#Display any exception conditions\n",
    "except Error as e:\n",
    "    print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that the WBGeoCodes table has been created, we can insert the data values contained in the geocodes_df data frame in the  database table.\n",
    "\n",
    "\n",
    "**Activity 2:**  Write an INSERT INTO statement to insert the data from the geocodes_df dataframe into the WBGeoCodes table. Given that the column order of data in the data frame matches the WBGeoCodes table schema order you can use just a VALUES argument with positional parameters for the three values. Starter code is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Activity 2\n",
    "# --------------\n",
    "\n",
    "# InsertIntoWBGeoCodesStmt = 'INSERT INTO ???'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we will again open a database connection and cursor then execute the INSERT INTO statement and use the Pandas\n",
    "# :iteratuples method for each row in the data frame.  This is used in our example on the GeoCodes data since it is a small\n",
    "# data frame and offered as a point of comparison to other methods on data insertion we will be using later.  This method\n",
    "# can be slow in performance.\n",
    "\n",
    "try:\n",
    "    databaseFilename = 'WBIndicatorsDB01.db'\n",
    "    connection = sqlite3.connect(databaseFilename)\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    for row in geocodes_df.itertuples(index=False):\n",
    "        cursor.execute(InsertIntoWBGeoCodesStmt,row) # <--reference to the insert into statement created above.\n",
    "\n",
    "    # Save (commit) the changes\n",
    "    connection.commit()\n",
    "\n",
    "    connection.close()\n",
    "    print('Geocodes inserted in to the WBGeoCodes table.')\n",
    "\n",
    "except Error as e:\n",
    "    print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Activity 3:** \n",
    "After you have successfully inserted the GeoCodes into the WBGeoCodes table, **Rerun** the above cell again.  Is there an outcome exception condition printed?  Explain the exception condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity 3: Explain exception condition - Answer:\n",
    "#---------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 4:**  \n",
    "We have inserted the data into the WBGeoCodes table so now let's take a look at it using a SELECT statement.\n",
    "Write an SQL **Select** statement to retrieve and display all the rows and fields of GeoCodes data from the WBGeoCodes table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Activity 4\n",
    "# --------------\n",
    "\n",
    "# -- Write a Select statement to retrieve and display all the rows of GeoCodes from the WBGeoCodes table.\n",
    "\n",
    "#select_GeoCodesStmt = \"SELECT ??? \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again a database connection and cursor is established and then the SELECT statement is execute all inside the try/catch block.\n",
    "# Notice we using simple iteration (for loop) to display the data. This is for comparison purposes and can be slow in performance\n",
    "# and generate a long listing for large data sets.\n",
    "\n",
    "try:\n",
    "    databaseFilename = 'WBIndicatorsDB01.db'\n",
    "    connection = sqlite3.connect(databaseFilename)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    cursor.execute(select_GeoCodesStmt) # <--reference to the SELECT statement created above.\n",
    "    \n",
    "    #Generate a list of tuples for all rows of data\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    #for each single row in all the rows, print each row one-by-one\n",
    "    print(\"All GeoCodes\")\n",
    "    for row in rows:\n",
    "        print(row)   \n",
    "\n",
    "    connection.close()\n",
    "    \n",
    "except Error as e:\n",
    "    print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity 5**: The above code segment uses iteration to display the results of the SQL **Select** query you wrote in **Activity 4**.  What recommendations can you provide to eliminate the iterative \"for loop\" and potentially enhance performance?  Please provide your answer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity 5 - Answer:\n",
    "#--------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Challenge 1\n",
    "\n",
    "We have successfully created the WBGeoCodes table, inserted the data values database table. Now we will perform similar steps for handling the the cell phone use and traffic fatality data.  First we need to create a new single table in the database that will store the values from each of the two data frames.  \n",
    "\n",
    "Using the **[World Bank Database Dictionary](../resources/WorldBankDataDictionary.pdf)**, fill-in the necessary details for constructing a **CREATE TABLE** statement string for creating the **WBInducatorValues** table. Be sure to **specify each column name and data type** and set the **primary key** using all the required fields. To enforce the **referential integrity constrait** (a 1 - many relationship) between the WBGeoCodes and WBIndicatorValues tables, a statement specifying the **FOREIGN KEY** is required.\n",
    "\n",
    "\n",
    "**Activity Challenge 1:** Write a CREATE TABLE statement based on the above specifications. Starter code is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Activity Challenge 1\n",
    "# --------------\n",
    "#WBIndicatorValues_CreateTableStmt = \"CREATE TABLE ??? ( \"\n",
    "#WBIndicatorValues_CreateTableStmt += \" ??? \"\n",
    "#WBIndicatorValues_CreateTableStmt += \" )\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first create the new table in the database using the newly constructed CREATE TABLE statement and then in the following cells, reshape the indicator data frames and insert the data into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    databaseFilename = 'WBIndicatorsDB01.db'\n",
    "    connection = sqlite3.connect(databaseFilename)\n",
    "    cursor = connection.cursor()\n",
    "        \n",
    "    cursor.execute(\"DROP TABLE if exists WBIndicatorValues;\")            \n",
    "    \n",
    "    #Following statement required to enable foreign key enforcement\n",
    "    print(connection.execute(\"PRAGMA foreign_keys\"))\n",
    "    print(connection.execute(\"PRAGMA foreign_keys = ON\"))\n",
    "\n",
    "    cursor.execute(WBIndicatorValues_CreateTableStmt)\n",
    "    connection.commit()\n",
    "    \n",
    "    display_db_table_structure(connection) #<--generates below output\n",
    "\n",
    "    connection.close()\n",
    "    print(\"WBIndicatorValues table created.\")\n",
    "    \n",
    "except Error as e:\n",
    "    print(\"Exception: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity Challenge 2\n",
    "\n",
    "**Activity Challenge 2:** Succinctly discuss or explain the output results printed using the \"display_db_table_structure(connection)\" function above: What does this information tell us about the database and tables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activity Challenge 2 - Answer:\n",
    "#----------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Let's create a LIST and iterate over each one to view the cellphone and mortality data contained in the data frames.\n",
    "#-- We will reference this list for performing the insert of data into the WBIndicatorValues table in the following cell.\n",
    "\n",
    "df_indicator_LIST = [TrafficMort_df,CellPhones_df]\n",
    "for indicator_df in df_indicator_LIST:\n",
    "    display(indicator_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a database schema available that is constructed to establish a 1:many relationship between the GeoCodes and Indicator data, we need to reshape our two original data frame into a structure that matches the schema of the WBIndicatorValues table.  We will do this reshaping using the Pandas .melt() method which essentially reshapes a wide structure to a long structure.  \n",
    "\n",
    "The wide structure as shown above contains separate columns for each year's indicator value, but our table schema has a field for the indicator code and year so a reshaping is necessary in order to insert the data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's use melt (our wide to long) data frame\n",
    "\n",
    "try:\n",
    "    databaseFilename = 'WBIndicatorsDB01.db'\n",
    "    connection = sqlite3.connect(databaseFilename)\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"DROP TABLE if exists WBIndicatorValues;\")\n",
    "    cursor.execute(WBIndicatorValues_CreateTableStmt)\n",
    "    \n",
    "    display_db_table_structure(connection)\n",
    "    \n",
    "    df_indicator_LIST = [TrafficMort_df,CellPhones_df]\n",
    "\n",
    "    #iterate over each original indicator data frame\n",
    "    for data_df in df_indicator_LIST:\n",
    "        #The melt will hold the id_vars list unchanged. We set the var_name, and value_name for the names of the new colmuns specified in the value_vars list\n",
    "        temp_melted_df = pd.melt(data_df, id_vars =['CountryRegion3Code', 'IndicatorName', 'IndicatorCode'],  var_name='Year', value_name ='VALUE', value_vars =['Y2000','Y2019'])     \n",
    "        \n",
    "        #Let Pandas insert the melted data frame into the WBIndicatorValues table -- high performance, no loops\n",
    "        temp_melted_df.to_sql(name='WBIndicatorValues', if_exists='append', index=False, con=connection)\n",
    "        \n",
    "    connection.close()\n",
    "    print(\"Original indicator data inserted into the WBIndicatorValues table.\")\n",
    "except Error as e:\n",
    "    print(\"Exception: \", e)\n",
    "\n",
    "#display the melted data frame for comparison purposes\n",
    "print('Melted DF')\n",
    "display(temp_melted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Challenge 3 \n",
    "\n",
    "**Activity Challenge 3:** Write a SELECT statement to bring together (join) the geocode and indicator tables in the FROM clause and result values using a WHERE clause.\n",
    "\n",
    "The resulting data frame should contain all rows (266) and the following columns:\n",
    "\n",
    "CountryRegion3Code CountryRegionName IsCountry IndicatorCode Year VALUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for Activity Challenge 3\n",
    "# ----------------------------------\n",
    "\n",
    "#SELECT_all_dataStmt = \"SELECT \"\n",
    "#SELECT_all_dataStmt += \" ??? \"\n",
    "#SELECT_all_dataStmt += \"FROM ??? \"\n",
    "#SELECT_all_dataStmt += \"WHERE ??? \"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUERY the table values\n",
    "\n",
    "try: \n",
    "    databaseFilename = 'WBIndicatorsDB01.db'\n",
    "    connection = sqlite3.connect(databaseFilename)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    all_data_df = pd.read_sql_query(SELECT_all_dataStmt, connection) #<-- execute the SELECT statement.\n",
    "\n",
    "    connection.close()\n",
    "\n",
    "except Error as e:\n",
    "    print(\"Exception: \", e)\n",
    "\n",
    "#Display the result data frame containing all the data from the database\n",
    "display(all_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since our data now stored in the WBIndicatorValues table is in a relational structure, we will extract a subset of rows for only the individual countries (ignoring the world regions) and perform additional reshaping to get it into a structure that is easily analyzed and graphed. For this we will have to construct a new column containing the concatenation of the indicator code and year, drop some columns, and then reshape the remaining data into a wide structure using the pandas .pivot() method. Essentially this is like and \"unmelt\" operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Subset rows from all data for just the counties\n",
    "CountryOnly_DF=all_data_df[all_data_df['IsCountry'] == 1]\n",
    "\n",
    "#Extract only the 4 listed colums that form the basis for analysis\n",
    "CountryOnly_DF=CountryOnly_DF.loc[:,['CountryRegion3Code','IndicatorCode','Year','VALUE']]        \n",
    "\n",
    "\n",
    "#Create a new column containing the concatenation of the indicator code and year, then drop those two columns\n",
    "CountryOnly_DF['IndicatorYear']=CountryOnly_DF['IndicatorCode']+'_'+CountryOnly_DF['Year'].astype(\"str\")\n",
    "CountryOnly_DF.drop(['IndicatorCode','Year'], axis=1, inplace=True)\n",
    "\n",
    "#Create a new data frame in the wide format that will pivot the new \"IndicatorCode\" values new columns\n",
    "CountryOnly_pivot_DF = CountryOnly_DF.pivot(index='CountryRegion3Code', columns='IndicatorYear', values='VALUE')\n",
    "\n",
    "\n",
    "#Reset the data frame index\n",
    "CountryOnly_pivot_DF = CountryOnly_pivot_DF.reset_index()\n",
    "\n",
    "#Display the pivoted data frame\n",
    "display(CountryOnly_pivot_DF)\n",
    "\n",
    "#Compute the correlations between the 4 indicator code values columns\n",
    "CountryOnly_pivot_DF.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import display, Markdown #<--this import permits displaying markdown as part of the output\n",
    "\n",
    "\n",
    "#Create some descriptive exploratory univariate and bi-variate plots\n",
    "\n",
    "#Display some markdown in the output sell\n",
    "display(Markdown(\"## Exploratory Plots for Country Cell Phone Use and Traffic Fatalities - 2000 and 2019\"))\n",
    "\n",
    "#Create histograms for each indactor code value column\n",
    "hist = CountryOnly_pivot_DF.hist(bins=30)\n",
    "\n",
    "#Create basic scatter plot for each indicator code year column pairs \n",
    "ax1 = CountryOnly_pivot_DF.plot.scatter(x='IT.CEL.SETS.P2_Y2000',\n",
    "                      y='SH.STA.TRAF.P5_Y2000',\n",
    "                      c='DarkBlue',\n",
    "                      title='Country Cell Phone Use by Traffic Fatalites - Year: 2000')\n",
    "\n",
    "ax2 = CountryOnly_pivot_DF.plot.scatter(x='IT.CEL.SETS.P2_Y2019',\n",
    "                      y='SH.STA.TRAF.P5_Y2019',\n",
    "                      c='DarkRed',\n",
    "                      title='Country Cell Phone Use by Traffic Fatalites - Year: 2019')  \n",
    "                                     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusion:\n",
    "\n",
    "\n",
    "We have walk through many of the DSA-PLC stages in this mini-analysis practice. The problem has been briefly define, data has been acquired and attributed to its original source, much data carpentry and reshaping was executed, and expletory data analysis (EDA) was performed.\n",
    "\n",
    "The results of the EDA show the indicator variables for individual countries to be skewed and multi-modal with some potential extreme outlier values. The scatter plots show the result of these summary descriptors as do the only modest correlations strength.\n",
    "\n",
    "More examination of the data is needed to better prepare it for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your notebook, then `File > Close and Halt`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
